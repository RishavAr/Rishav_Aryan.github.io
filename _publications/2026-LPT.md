---
title: "Latent-to-Parameter Transfer: High-Bandwidth Logic Injection via Neural-State Conditioned Hypernetworks"
collection: publications
category: in_progress
date: 2025-12-25
venue: "NeurIPS 2026 (Planned Submission)"
paperurl: ""
codeurl: ""
excerpt: "ðŸ”µ Research in Progress. Upcoming NeurIPS submission on Latent-to-Parameter Transfer (LPT), a hypernetwork-based framework that conditions on teacher latent states to generate adaptive LoRA parameters for efficient reasoning transfer."
---


**Keywords:** Knowledge Distillation, Hypernetworks, LoRA, Reasoning Models, Parameter-Efficient Transfer

This work proposes **Latent-to-Parameter Transfer (LPT)**, a framework that leverages internal latent representations of a frozen teacher model to generate task-specific low-rank parameter updates for a student model. Unlike text-conditioned adapter generation, LPT exploits the **high-dimensional geometry of the teacherâ€™s reasoning process**, enabling more faithful logic transfer. 

_Code and preprint will be released upon submission._

