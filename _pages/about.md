---
layout: single
title: "About"
permalink: /
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

## 
I am an **AI / ML Researcher & Engineer** working at the intersection of
**representation learning, attention mechanisms, and adaptive language models**.
My work focuses on understanding *how* modern models learn and reason, and
translating those insights into **end-to-end, executable systems**.

I currently work as a **Research Assistant in Machine Learning and Reinforcement Learning** at George Mason University under Professor K.C.Chang, while continuing independent research and systems development in parallel.

I operate as an **independent researcher**, capable of taking ideas from
theory ‚Üí experiments ‚Üí systems ‚Üí communication, while remaining open to
collaboration with researchers who share similar rigor and depth.


---

## üß† Research Direction
My research interests include:
- Self-adapting language models  
- Attention and hypernetwork-based architectures  
- Representation transfer beyond standard knowledge distillation  
- Implicit regularization and generalization in deep models  

I am actively developing research contributions aligned with
**NeurIPS / ICML‚Äìstyle venues**, emphasizing originality, clarity,
and empirical grounding.

---

## üìÑ Active Research Work
**Latent-to-Parameter Transfer (LPT)**  
I am developing a research framework that studies how **reasoning structure can be transferred from large language models to smaller models** via **neural-state‚Äìconditioned hypernetworks**, rather than output-level supervision.

This work explores:
- Conditioning on internal latent states of a frozen teacher model  
- Generating low-rank parameter updates (LoRA) dynamically  
- Online (per-input) and offline (consolidated) transfer modes  
- Improved efficiency in FLOPs while preserving reasoning accuracy  
- Near-zero catastrophic forgetting in continual learning settings  

The broader goal is to move beyond bandwidth-limited distillation and toward
**process-level knowledge transfer**.

---

## üöÄ Startup Work ‚Äî QSTACK
Alongside research, I am building **QSTACK**, an early-stage systems startup
focused on **intelligent orchestration of heterogeneous compute for machine
learning workloads**.

QSTACK investigates:
- Runtime-aware CPU/GPU execution routing  
- Cost- and latency-aware scheduling decisions  
- Treating heterogeneous hardware as a unified execution substrate  

This work is deeply systems-driven and complements my research by grounding
theoretical insights in real execution constraints.

---

## üîÅ Weekly Research & Implementation Releases
I maintain a **weekly research-to-implementation cadence**, where I:
- Study recent **NeurIPS / ICML / ICLR papers**
- Publish structured technical breakdowns
- Implement ideas **end-to-end from first principles**
- Release code, experiments, and insights publicly

Recent and upcoming focus areas include:
- Self-Adapting Language Models  
- Attention and Manifold-Constrained Learning  
- Implicit Regularization in Diffusion Models  
- Model Context Protocol (MCP) for agent interoperability  
- Reinforcement-learning-based reasoning systems  
- Hierarchical and multi-agent architectures  

Each release emphasizes **mechanisms, mathematics, and system behavior**‚Äî
not surface-level summaries.

---

## üßë‚Äç‚öñÔ∏è Academic Service
- **Reviewer (3√ó)**, *Expert Systems with Applications*
